---
title: | 
  | Evaluation des algorithmes heuristiques pour le voyageur de commerce
  | avec les méthodes d'insertion
  | ![](Images/logo_lamme.png){width=1in}  ![](Images/logo_UEVE.png){width=1.7in}
  |  M2 Data Science Algorithmique
  | Exemple de projet
author:  "Vincent Runge"
date: "jeudi 27 octobre 2022"
header-includes:
  - \usepackage[french]{babel}
output:
  pdf_document:
    keep_tex: yes
    toc: true
  html_document:
    df_print: paged
urlcolor: blue
---

\noindent\hrulefill


# Description du problème et objectif

On tire de manière aléatoire dans le carré unité selon une loi uniforme $\mathcal{N}(0,1) \times \mathcal{N}(0,1)$ un nombre `n` de villes. On donne ici un exemple avec 40 villes

```{r}
n <- 40
villes <- matrix(runif(2*n), n, 2)
```


```{r, echo=FALSE}
plot(villes[,1], villes[,2], xlim = c(0,1), ylim = c(0,1), xlab = "", ylab = "", asp=1, pty ="s")
```

Notre premier objectif est de contruire un "plus court chemin" par un **algorithme heuristique**. On comparera différentes **méthodes d'insertion** et on analysera leur **temps de calcul** numériquement. 

**Nos objectifs : **

- comparer les performances en temps des algorithmes

- évaluer la distance à la solution optimale 

- pour cela coder l'algorithme de *branch and bound*. 


On note $c(i,j)$ le coût pour passer de la ville $i$ à la ville $j$. Notre objectif est de trouver la permutation des indices $(1,...,n)$ notée $(v_1,...,v_n)$ qui minimisera la longueur du tour:

$$\sum_{i=1}^{n}c(v_i,v_{i+1})$$
avec $v_{n+1} = v_1$. Remarquez bien qu'une permutation contient une et une seule fois chaque indice de sorte que le tour est complet et passe bien par chaque ville une et une seule fois.



# L'algorithme naïf du plus proche voisin

C'est la méthode la plus simple. Elle consiste à partir d'une ville $i$ et de contruire le chemin de proche en proche en ajoutant en bout de chemin la ville la plus proche parmi les villes non explorées. Quand toutes les villes sont explorées on revient à la première ville pour fermer le tour.

On peut répéter la procédure pour chaque ville de départ (on exécute ainsi $n$ fois cette méthode) pour choisir le meilleur chemin parmi les $n$ obtenus. 

(librairies à installer)
```{r}
library(ggplot2) #ggplot
library(reshape2) #melt
library(parallel) #mclapply
```

```{r}
library(TSP)
res1 <- NN_TSP(villes)
plot(tour = res1, data = villes)
(t1 <- tour_length(res1, villes))
```

**EXERCICE :** pour le problème euclidien du voyageur de commerce (inégalité triangulaire respectée), le tour optimal ne peut pas contenir de croisement. **Le prouver!**


# Les algorithmes d'insertion

les algorithmes d'insertion consistent à insérer les villes l'une après l'autre dans un tour partiel (contenant qu'un sous-ensemble des villes) partant d'une ou deux villes de départ.

## L'algorithme d'insertion *cheapest* ("le moins cher")

Pour un tour partiel déjà constitué on cherche l'arrête (le couple de villes) $(i,j)$ et la ville encore non incluse $k$ qui minimise la quantité

$$c(i,k) + c(k,j) - c(i,j)$$
C'est ainsi l'insertion la moins coûteuse. On pourra aussi répéter l'algorithme pour chacune des villes de départ.

```{r}
res2 <- greedy_TSP_best(villes)
plot(tour = res2, data = villes)
(t2 <- tour_length(res2, villes))
```



## L'algorithme d'insertion *nearest* ("le plus proche")

Pour un tour partiel déjà constitué on cherche la ville $i$ et la ville encore non incluse $k$ qui minimise la quantité $c(i,k)$ : c'est la ville la plus proche du tour. Une fois trouvée on insert cette ville à sa position optimale en trouvant l'arrête $(i,j)$ qui minimise $c(i,k) + c(k,j) - c(i,j)$
C'est ainsi l'insertion la plus proche. On pourra aussi répéter l'algorithme pour chacune des villes de départ.

```{r}
res3 <- greedy_TSP_min(villes)
plot(tour = res3, data = villes)
(t3 <- tour_length(res3, villes))
```


## L'algorithme d'insertion *farthest* ("le plus éloigné")

Pour un tour partiel déjà constitué on cherche pour chaque ville non encore incluse $k$, la ville $i$ du tour la plus proche. On obtient des distances $c(i,k)$ avec autant de couples $(i,k)$ qu'il y a de villes non incluses. On sélectionne le plus grande de ces distances et la ville $k$ qui lui est associée. On insère cette ville $k$ à sa position optimale selon le critère habituel (min de $c(i,k) + c(k,j) - c(i,j)$). 


```{r}
res4 <- greedy_TSP_max(villes)
plot(tour = res4, data = villes)
(t4 <- tour_length(res4, villes))
```

Affichés tous ensemble :

```{r, echo=FALSE}
par(mfrow=c(2,2),
    oma = c(5,4,0,0) + 0.1,
    mar = c(1.5,1.5,1,0) + 0.1)
plot(tour = res1, data = villes, main = "NN", value = t1)
plot(tour = res2, data = villes, main = "best", value = t2)
plot(tour = res3, data = villes, main = "min", value = t3)
plot(tour = res4, data = villes, main = "max", value = t4)
```

Il est possible dans ce cadre eucliden d'obtenir une [bornes sur la longueur du tour](http://www.cs.albany.edu/~res/tsp_sicomp_1977.pdf). Ces algorithmes heuristiques sont donc des algorithmes d'approximation (sauf peut-être pour *farthest*)!


$$algo(cheapest) \le 2 \,algo(opt)$$

$$algo(closest) \le 2 \,algo(opt)$$


$$algo(farthest) \le (\lceil log_2(n) \rceil + 1) \,algo(opt)$$



# Comparaison des performances 

## Pour les différents algorithmes heuristiques

On répète 100 fois les 4 algorithmes sur des données générées par $\mathcal{U}[0,1] \times \mathcal{U}[0,1]$
```{r, echo = FALSE, eval=TRUE}
dist <- data.frame(matrix(0,100,4))
colnames(dist) <- c("NN", "best", "min", "max")

for(i in 1:100)
{
  n <- 40
  villes <- matrix(runif(2*n), n, 2)
  
  t1 <- tour_length(NN_TSP(villes), villes)
  t2 <- tour_length(greedy_TSP_best(villes), villes)
  t3 <- tour_length(greedy_TSP_min(villes), villes)
  t4 <- tour_length(greedy_TSP_max(villes), villes)
  
  dist[i,] <- c(t1,t2,t3,t4)
}
```

```{r,echo = FALSE, eval=TRUE}
df <- melt(dist)
ggplot(df, aes(x=variable, y=value)) + geom_violin()
```

Rang moyen :

```{r, echo=FALSE}
colMeans(t(apply(dist,1,rank)))
```


## Pour une distribution normale des villes


On répète 100 fois les 4 algorithmes sur des données normales générées par $\mathcal{N}(0,1) \times \mathcal{N}(0,1)$

```{r, echo = FALSE, eval=TRUE}
dist <- data.frame(matrix(0,100,4))
colnames(dist) <- c("NN", "best", "min", "max")

for(i in 1:100)
{
  n <- 40
  villes <- matrix(runif(2*n), n, 2)
  
  t1 <- tour_length(NN_TSP(villes), villes)
  t2 <- tour_length(greedy_TSP_best(villes), villes)
  t3 <- tour_length(greedy_TSP_min(villes), villes)
  t4 <- tour_length(greedy_TSP_max(villes), villes)
  
  dist[i,] <- c(t1,t2,t3,t4)
}
```

```{r,echo = FALSE, eval=TRUE}
df <- melt(dist)
ggplot(df, aes(x=variable, y=value)) + geom_violin()
```

Rang moyen :

```{r,echo = FALSE, eval=TRUE}
colMeans(t(apply(dist,1,rank)))
```


**EXERCICE :** Comment évoluent ces résultats si on répète chaque algorithme pour les $n$ initialisations possibles?

REPONSE (distribution uniforme): 

```{r, echo = FALSE, eval=TRUE}
dist <- data.frame(matrix(0,100,4))
colnames(dist) <- c("NN", "best", "min", "max")

for(i in 1:100)
{
  n <- 40
  villes <- matrix(runif(2*n), n, 2)
  
  t1 <- tour_length(NN_TSP(villes, type = "all"), villes)
  t2 <- tour_length(greedy_TSP_best(villes, type = "all"), villes)
  t3 <- tour_length(greedy_TSP_min(villes, type = "all"), villes)
  t4 <- tour_length(greedy_TSP_max(villes, type = "all"), villes)
  
  dist[i,] <- c(t1,t2,t3,t4)
}
```

```{r,echo = FALSE, eval=TRUE}
df <- melt(dist)
ggplot(df, aes(x=variable, y=value)) + geom_violin()
```

Rang moyen :

```{r, echo=FALSE}
colMeans(t(apply(dist,1,rank)))
```



# Temps de calcul

On étudie ici le temps la complexité des algorithmes en fonction du nombre $n$ de villes. 

On définit une fonction de type `one.simu` qui simule une seule expérience pour un choix de ville.

```{r}
one.simu_time_TSP <- function(i, data, algo = "NN", type = "one")
{
  if(algo == "NN")
  {
    start_time <- Sys.time()
    NN_TSP(data, type = type)
    end_time  <- Sys.time()
  }
  if(algo == "best")
  {
    start_time <- Sys.time()
    greedy_TSP_best(data, type = type)
    end_time  <- Sys.time()
  }
  if(algo == "min")
  {
    start_time <- Sys.time()
    greedy_TSP_min(data, type = type)
    end_time  <- Sys.time()
  }
  if(algo == "max")
  {
    start_time <- Sys.time()
    greedy_TSP_max(data, type = type)
    end_time  <- Sys.time()
  }
  return(unclass(end_time - start_time)[1])
}
```


On construit un vecteur de taille de ville selon une échelle logarithmique

```{r}
my_n_vector_LOG <- seq(from = log(10), to = log(100), by = log(10)/40)
my_n_vector <- round(exp(my_n_vector_LOG))
my_n_vector
diff(log(my_n_vector))
```

On construit un data frame qui contiendra les résultats
```{r}
p <- 50 ### répétition
df <- data.frame(matrix( nrow = 4 * length(my_n_vector), ncol = 2 + p))
colnames(df) <- c("type", "n", 1:p)
dim(df)
```

On lance la simulation sur plusieurs coeurs.

```{r}
nbCores <- 8
j <- 1

for(n in my_n_vector)
{
  print(n)
  liste1 <- mclapply(1:p, FUN = one.simu_time_TSP,
                      data = matrix(runif(2*n), n, 2),
                     algo = "NN",
                     mc.cores = nbCores)

  liste2 <- mclapply(1:p, FUN = one.simu_time_TSP,
                     data = matrix(runif(2*n), n, 2),
                     algo = "best",
                     mc.cores = nbCores)

  liste3 <- mclapply(1:p, FUN = one.simu_time_TSP,
                     data = matrix(runif(2*n), n, 2),
                     algo = "min",
                     mc.cores = nbCores)

  liste4 <- mclapply(1:p, FUN = one.simu_time_TSP,
                     data = matrix(runif(2*n), n, 2),
                     algo = "max",
                     mc.cores = nbCores)


  df[j ,] <- c("NN", n, do.call(cbind, liste1))
  df[j+1 ,] <- c("best", n, do.call(cbind, liste2))
  df[j+2 ,] <- c("min", n, do.call(cbind, liste3))
  df[j+3 ,] <- c("max", n, do.call(cbind, liste4))
  j <- j + 4
}

df <- melt(df, id.vars = c("type","n"))
```

tranformations techniques : 

```{r}
data_summary <- function(data, varname, groupnames)
{
  require(plyr)
  summary_func <- function(x, col)
  {
    c(mean = mean(x[[col]], na.rm=TRUE),
      q1 = quantile(x[[col]], 0.025), q3 = quantile(x[[col]], 0.975))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- rename(data_sum, c("mean" = varname))
  return(data_sum)
}

df2 <- df
df2[,2] <- as.double(df[,2])
df2[,3] <- as.double(df[,3])
df2[,4] <- as.double(df[,4])
summary(df2)

df_new <- data_summary(df2, varname="value",
                           groupnames=c("type","n"))

theMin <- min(df_new[,3:5],df_new[,3:5])
theMax <- max(df_new[,3:5],df_new[,3:5])
```


On trace différentes courbes. 

```{r}
ggplot(df_new, aes(x = n, y = value, col=type)) +  scale_x_log10()+
  scale_y_log10(limits = c(theMin, theMax))  +
  labs(y = "time in seconds") +  labs(x = "number of cites") +
  geom_point(size = 2, aes(shape = type)) +
  geom_errorbar(aes(ymin=`q1.2.5%`, ymax=`q3.97.5%`), width=.01) +
  scale_colour_manual(values = c("best" = "#0080FF",
                                 "max" = " dark blue", "min" = "blue", "NN" = "red")) +
  theme(axis.text.x = element_text(size=15),
        axis.text.y = element_text(size=15),
        legend.text=element_text(size=15),
        axis.title.x=element_text(size=15),
        axis.title.y=element_text(size=15),
        legend.position = c(0.7, 0.1),
        legend.title = element_blank())
```


On calcule les valeurs des coefficients directeurs.

Pour NN :

```{r}
R1 <- df_new[df_new$type == "NN",c(2,3)]
l1 <- lm(log(value) ~ log(n), data = R1, )
summary(l1)
l1$coefficients
```

Pour best : 

```{r}
R2 <- df_new[df_new$type == "best",c(2,3)]
l2 <- lm(log(value) ~ log(n), data = R2, )
summary(l2)
l2$coefficients
```

Pour min :

```{r}
R3 <- df_new[df_new$type == "min",c(2,3)]
l3 <- lm(log(value) ~ log(n), data = R3, )
summary(l3)
l3$coefficients
```
 
 
Pour max : 


```{r}
R4 <- df_new[df_new$type == "max",c(2,3)]
l4 <- lm(log(value) ~ log(n), data = R4, )
summary(l4)
l4$coefficients
```


# Amélioration de tour par 2-opt et 3-opt

**EXERCICE :**

- Ajouter les fonctions `opt2` et `opt3` à coder

- Evaluer l'amélioration apportée en terme de distance


# Algorithme *Branch and Bound*

- Ajouter la fonction `B_and_B`

- Evaluer le coefficient d'approximation des méthodes dans le cas d'une répartition uniforme et normale des villes




